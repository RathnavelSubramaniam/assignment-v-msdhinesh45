{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5f8270",
   "metadata": {},
   "source": [
    "Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9546a",
   "metadata": {},
   "source": [
    "1.We will now try to predict per capita crime rate in the Boston data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a152388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0e31a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbffed1",
   "metadata": {},
   "source": [
    "(a)\tTry out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18f5fd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (404, 12) (102, 12)\n",
      "Columns: ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "\n",
    "X = boston.data\n",
    "y = X['CRIM']   # per capita crime rate\n",
    "X = X.drop(columns=['CRIM'])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Shape:\", X_train.shape, X_test.shape)\n",
    "print(\"Columns:\", X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d43ce6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              features          aic          bic    adj_r2\n",
      "0        (ZN, NOX, DIS, RAD, B, LSTAT)  2708.736269  2736.746173  0.403544\n",
      "1                      (RAD, B, LSTAT)  2709.046702  2725.052362  0.398698\n",
      "2       (ZN, CHAS, DIS, RAD, B, LSTAT)  2709.224642  2737.234546  0.402822\n",
      "3  (ZN, CHAS, NOX, DIS, RAD, B, LSTAT)  2709.224882  2741.236201  0.404270\n",
      "4             (ZN, DIS, RAD, B, LSTAT)  2709.300809  2733.309299  0.401253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure numeric only\n",
    "X_train_clean = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "y_train_clean = pd.to_numeric(y_train, errors='coerce')\n",
    "\n",
    "# Drop rows with missing values (if any appear after conversion)\n",
    "X_train_clean = X_train_clean.dropna()\n",
    "y_train_clean = y_train_clean.loc[X_train_clean.index]\n",
    "\n",
    "def best_subset_selection(X, y, max_features=13):\n",
    "    results = []\n",
    "    for k in range(1, max_features+1):\n",
    "        for combo in combinations(X.columns, k):\n",
    "            X_model = sm.add_constant(X[list(combo)].astype(float))  # force numeric\n",
    "            model = sm.OLS(y.astype(float), X_model).fit()\n",
    "            results.append({\n",
    "                'features': combo,\n",
    "                'aic': model.aic,\n",
    "                'bic': model.bic,\n",
    "                'adj_r2': model.rsquared_adj\n",
    "            })\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values('aic').reset_index(drop=True)\n",
    "\n",
    "best_subsets = best_subset_selection(X_train_clean, y_train_clean, max_features=13)\n",
    "print(best_subsets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc774b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZN          float64\n",
      "INDUS       float64\n",
      "CHAS       category\n",
      "NOX         float64\n",
      "RM          float64\n",
      "AGE         float64\n",
      "DIS         float64\n",
      "RAD        category\n",
      "TAX         float64\n",
      "PTRATIO     float64\n",
      "B           float64\n",
      "LSTAT       float64\n",
      "dtype: object\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe9009",
   "metadata": {},
   "source": [
    "1. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cec43590",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_train = X_train.dropna()\n",
    "X_test = X_test.dropna()\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_test = y_test.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68f0539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (Ridge): 0.6135907273413176\n",
      "Test MSE (Ridge): 23.806517688658715\n"
     ]
    }
   ],
   "source": [
    "alphas = 10**np.linspace(-3, 3, 100)\n",
    "ridge = RidgeCV(alphas=alphas)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "\n",
    "print(\"Best alpha (Ridge):\", ridge.alpha_)\n",
    "print(\"Test MSE (Ridge):\", ridge_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "460952a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZN         float64\n",
      "INDUS      float64\n",
      "CHAS         int64\n",
      "NOX        float64\n",
      "RM         float64\n",
      "AGE        float64\n",
      "DIS        float64\n",
      "RAD          int64\n",
      "TAX        float64\n",
      "PTRATIO    float64\n",
      "B          float64\n",
      "LSTAT      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921aa4ea",
   "metadata": {},
   "source": [
    "2. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b52a593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (Lasso): 0.8486933195540266\n",
      "Selected Features: 6\n",
      "Test MSE (Lasso): 24.532353321703862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1632: FutureWarning: 'alphas=None' is deprecated and will be removed in 1.9, at which point the default value will be set to 100. Set 'alphas=100' to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(alphas=None, cv=10, max_iter=10000, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "print(\"Best alpha (Lasso):\", lasso.alpha_)\n",
    "print(\"Selected Features:\", np.sum(lasso.coef_ != 0))\n",
    "print(\"Test MSE (Lasso):\", lasso_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47948db",
   "metadata": {},
   "source": [
    "3. Principal Component Regression (PCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82740807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best #components: 12\n",
      "Test MSE (PCR): 23.864882518234275\n"
     ]
    }
   ],
   "source": [
    "mse_pcr = []\n",
    "for n in range(1, X_train.shape[1]+1):\n",
    "    pcr = make_pipeline(StandardScaler(), PCA(n_components=n), LinearRegression())\n",
    "    scores = cross_val_score(pcr, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "    mse_pcr.append(-scores.mean())\n",
    "\n",
    "best_ncomp = np.argmin(mse_pcr) + 1\n",
    "print(\"Best #components:\", best_ncomp)\n",
    "\n",
    "pcr_final = make_pipeline(StandardScaler(), PCA(n_components=best_ncomp), LinearRegression())\n",
    "pcr_final.fit(X_train, y_train)\n",
    "pcr_pred = pcr_final.predict(X_test)\n",
    "pcr_mse = mean_squared_error(y_test, pcr_pred)\n",
    "print(\"Test MSE (PCR):\", pcr_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd48b46",
   "metadata": {},
   "source": [
    " (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "955b168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = X['CRIM']   \n",
    "X = X.drop(columns=['CRIM'])\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.astype(float), y.astype(float), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe89ff",
   "metadata": {},
   "source": [
    "1. Linear Regression (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a456cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg_scores = cross_val_score(linreg, X_train, y_train,\n",
    "                                scoring='neg_mean_squared_error', cv=cv)\n",
    "linreg_mse_cv = -linreg_scores.mean()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "linreg_mse_test = mean_squared_error(y_test, linreg.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2db79f",
   "metadata": {},
   "source": [
    "2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c437fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(-3, 3, 100)\n",
    "ridge = RidgeCV(alphas=alphas, cv=cv, scoring='neg_mean_squared_error')\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "ridge_scores = cross_val_score(ridge, X_train, y_train,\n",
    "                               scoring='neg_mean_squared_error', cv=cv)\n",
    "ridge_mse_cv = -ridge_scores.mean()\n",
    "\n",
    "ridge_mse_test = mean_squared_error(y_test, ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9217b5",
   "metadata": {},
   "source": [
    "3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a5ce362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1632: FutureWarning: 'alphas=None' is deprecated and will be removed in 1.9, at which point the default value will be set to 100. Set 'alphas=100' to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(alphas=None, cv=cv, max_iter=10000, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lasso_mse_cv = np.min(lasso.mse_path_.mean(axis=1))\n",
    "lasso_mse_test = mean_squared_error(y_test, lasso.predict(X_test))\n",
    "selected_features = np.sum(lasso.coef_ != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce00858",
   "metadata": {},
   "source": [
    "4. Principal Component Regression (PCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Cross-Validation MSE (Train) ======\n",
      "Linear Regression: 47.7620\n",
      "Ridge Regression: 47.9502 (alpha=0.1748)\n",
      "Lasso Regression: 48.2221 (alpha=0.8487)\n",
      "PCR (best 12 comps): 47.7620\n",
      "\n",
      "====== Test Set MSE ======\n",
      "Linear Regression: 23.8649\n",
      "Ridge Regression: 23.8396\n",
      "Lasso Regression: 24.5324 (selected 6 features)\n",
      "PCR (best 12 comps): 23.8649\n"
     ]
    }
   ],
   "source": [
    "mse_pcr = []\n",
    "for n in range(1, X_train.shape[1] + 1):\n",
    "    pcr = make_pipeline(StandardScaler(), PCA(n_components=n), LinearRegression())\n",
    "    scores = cross_val_score(pcr, X_train, y_train,\n",
    "                             scoring='neg_mean_squared_error', cv=cv)\n",
    "    mse_pcr.append(-scores.mean())\n",
    "\n",
    "best_ncomp = np.argmin(mse_pcr) + 1\n",
    "pcr_final = make_pipeline(StandardScaler(), PCA(n_components=best_ncomp), LinearRegression())\n",
    "pcr_final.fit(X_train, y_train)\n",
    "pcr_mse_test = mean_squared_error(y_test, pcr_final.predict(X_test))\n",
    "\n",
    "print(\"====== Cross-Validation MSE (Train) ======\")\n",
    "print(f\"Linear Regression: {linreg_mse_cv:.4f}\")\n",
    "print(f\"Ridge Regression: {ridge_mse_cv:.4f} (alpha={ridge.alpha_:.4f})\")\n",
    "print(f\"Lasso Regression: {lasso_mse_cv:.4f} (alpha={lasso.alpha_:.4f})\")\n",
    "print(f\"PCR (best {best_ncomp} comps): {min(mse_pcr):.4f}\")\n",
    "\n",
    "print(\"\\n====== Test Set MSE ======\")\n",
    "print(f\"Linear Regression: {linreg_mse_test:.4f}\")\n",
    "print(f\"Ridge Regression: {ridge_mse_test:.4f}\")\n",
    "print(f\"Lasso Regression: {lasso_mse_test:.4f} (selected {selected_features} features)\")\n",
    "print(f\"PCR (best {best_ncomp} comps): {pcr_mse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8784bd",
   "metadata": {},
   "source": [
    " (c) Does your chosen model involve all of the features in the data set? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113110c",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "Linear Regression: Yes, it uses all features in X_train. Standard linear regression does not perform feature selection by itself, so every column contributes to the prediction.\n",
    "\n",
    "Ridge Regression: Yes, it also uses all features, but the coefficients are shrunk toward zero to reduce overfitting. Ridge does not set any coefficient exactly to zero—it only regularizes them.\n",
    "\n",
    "Lasso Regression: No, Lasso performs feature selection. It shrinks some coefficients exactly to zero depending on the value of alpha. In your case, selected_features = 6, meaning only 6 features out of the original set contribute to the final model. The rest are ignored.\n",
    "\n",
    "PCR (Principal Component Regression): Indirectly, PCR transforms all features into principal components first. The model then uses the best n components (here, 12 components) instead of the raw features. While all original features contribute to the components, the regression effectively reduces the dimensionality, so it does not use all features directly.\n",
    "\n",
    "\n",
    "Why or why not?\n",
    "\n",
    "Ridge: Uses all features because it only shrinks coefficients to reduce variance, not to eliminate predictors.\n",
    "\n",
    "Lasso: May ignore some features because its L1 penalty encourages sparsity, which can improve interpretability and reduce overfitting.\n",
    "\n",
    "PCR: Reduces dimensionality by combining correlated features into components, which captures most of the variance while ignoring less important directions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
